# comm4190_F25_Tutor_Task
This repository explores how structured prompting influences the way large language models (LLMs) perform in extended communicative tasks — specifically, acting as a tutor. The project compares two AI tutoring sessions (one unstructured and one using a structured “Mollick Tutor” prompt) on the topic *“How Social Media Influencers Construct Authenticity.”* A third, customized prompt was then developed and tested to refine tutoring style and communication effectiveness.

## Contents
- **Session 1 – Unstructured Tutor:** Baseline conversation without a structured prompt; demonstrates default AI tutoring behavior.  
- **Session 2 – Structured Tutor (Mollick Prompt):** Guided conversation using a pre-designed tutoring framework for comparison.  
- **New Prompt:** - Custom prompt incorporating communication and scaffolding improvements based on analysis of Sessions 1 and 2.  
- **Evaluation:** - My subjective and objective assessment of how tutoring language, tone, and adaptability changed across sessions.  
- **Testing LLMs as Authenticity Tutors.md:** - Project overview, methodology, and purpose.  

## Purpose
The goal of this project is to analyze how different levels of prompting structure can transform an LLM’s tutoring effectiveness — from merely delivering information to fostering genuine learning through questioning, responsiveness, and reflective dialogue.